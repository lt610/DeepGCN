<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="">
    <meta name="description" content="">
    <title>详细报告语句详情</title>
    <link href="../css/bootstrap.min.css" rel="stylesheet" />
    <link href="../css/style.css" rel="stylesheet" />
    <style type="text/css">
        .content-tips .paper-section{margin-bottom:0;min-width:450px;}
    </style>
</head>
<body>
<span id="gototop"></span>
<div>
    <div class="paper-txt P30">
        <div class="paper-section" style="margin-bottom:0;">
            <div class="font-bold MT5">
                该句相似度：<span class="g-font-color red similarNum">100%</span>
            </div>
            <div class="MT30">
                <p class="font-bold">您的语句：</p>
                <span>误差从输出层反向传播时，在每一层都要乘以该层的激活函数的导数。</span>
            </div>
        </div>
    </div>
    <div>
        <div class="tab-A">
            <ul class="tab-A-ul clearfix" tab-a="ul">
                                        <li class="active" data-id="学术期刊,学位论文,学术会议,书籍数据,互联网,自建库,all">
                            <span class="tab-text">综合</span>
                            <span class="tab-superscript">9</span>
                        </li>
                                    <li data-id="tab,学术期刊,学位论文,学术会议,书籍数据,自建库,local"
                                    >
                    <span class="tab-text">本地库</span>
                    <span class="tab-superscript">5</span>
                </li>
                                <li data-id="互联网,5">
                    <span class="tab-text">互联网</span>
                    <span class="tab-superscript">4</span>
                </li>
                            </ul>
        </div>
        <div class="none" tab="section" data-id="tab"
                    >
            <div class="tab-B clearfix">
                <div class="paper-section P30 PB0 clearfix" style="padding:30px 10px 0">
                <ul class="tab-B-ul pull-left clearfix" tab-b="ul">
                        <li class="active" data-id="tab,学术期刊,学位论文,学术会议,书籍数据,自建库,local">
                            <span class="tab-text">全部</span>
                            <span class="tab-superscript">5</span>
                            <span class="black-spacer"></span>
                        </li>
                        <li data-id="tab,学术期刊,1">
                            <span class="tab-text">期刊</span>
                            <span class="tab-superscript">1</span>
                            <span class="black-spacer"></span>
                        </li>
                        <li data-id="tab,学位论文,2">
                            <span class="tab-text">学位</span>
                            <span class="tab-superscript">4</span>
                            <span class="black-spacer"></span>
                        </li>
                        <li data-id="tab,学术会议,3">
                            <span class="tab-text">会议</span>
                            <span class="tab-superscript">0</span>
                            <span class="black-spacer"></span>
                        </li>
                        <li data-id="tab,书籍数据,4">
                            <span class="tab-text">图书</span>
                            <span class="tab-superscript">0</span>
                                                    </li>
                                            </ul>
                </div>
                <div class="g-line-row MT30 tab-pane-line"></div>
            </div>
        </div>
        <div class="content-tips">
            <div class="tab-content none" tab="section" data-id="1">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在期刊库共找出相似内容：<span>1</span>个                    <p>
                                        <div style="margin-top: 10px;">
                    <a href="#modify_suggest" class="g-btn g-btn-default g-btn-sm">查看修改意见</a>
                    </div>
                                    </div>
            </div>

            <div class="tab-content none" tab="section" data-id="2">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在学位库共找出相似内容：<span>4</span>个                    <p>
                                        <div style="margin-top: 10px;">
                    <a href="#modify_suggest" class="g-btn g-btn-default g-btn-sm">查看修改意见</a>                    </div>
                                    </div>
            </div>
            <div class="tab-content none" tab="section" data-id="3">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在会议库中没有找到与此句话相似的内容                    <p>
                                        <div style="margin-top: 10px;">
                                        </div>
                                    </div>
            </div>
            <div class="tab-content none" tab="section" data-id="4">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在图书库中没有找到与此句话相似的内容                    <p>
                                        <div style="margin-top: 10px;">
                                        </div>
                                    </div>
            </div>
            <div class="tab-content none" tab="section" data-id="5">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在互联网库共找出相似内容：<span>4</span>个                    <p>
                                        <div style="margin-top: 10px;">
                    <a href="#modify_suggest" class="g-btn g-btn-default g-btn-sm">查看修改意见</a>                    </div>
                                    </div>
            </div>
            <!-- 自建库 -->
            <div class="tab-content none" tab="section" data-id="6">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在自建库中没有找到与此句话相似的内容                    <p>
                                        <div style="margin-top: 10px;">
                                        </div>
                                    </div>
            </div>
                        <div class="tab-content" tab="section" data-id="all">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                                                在本地库和互联网库共找出相似内容：<span>9</span>个
                                            <p>
                                            <div style="margin-top: 10px;">
                    <a href="#modify_suggest" class="g-btn g-btn-default g-btn-sm">查看修改意见</a>

                    </div>
                                    </div>
            </div>
            <div class="tab-content none" tab="section" data-id="local">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在本地库共找出相似内容：<span>5</span>个                    <p>
                                        <div style="margin-top: 10px;">
                    <a href="#modify_suggest" class="g-btn g-btn-default g-btn-sm">查看修改意见</a>
                    </div>
                                    </div>
            </div>
        </div>



                <div class="tab-content" tab="section" data-id="互联网">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">1</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color red similarNum">100%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color red"><span class="g-underline-text">误差</span><span class="g-underline-text">从</span><span class="g-underline-text">输出</span><span class="g-underline-text">层</span><span class="g-underline-text">反向</span><span class="g-underline-text">传播</span><span class="g-underline-text">时</span>，<span class="g-underline-text">在</span><span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">误差</span><span class="g-underline-text">从</span><span class="g-underline-text">输出</span><span class="g-underline-text">层</span><span class="g-underline-text">反向</span><span class="g-underline-text">传播</span><span class="g-underline-text">时</span>，<span class="g-underline-text">在</span><span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            ）反向传播计算每一层的误差；（3）计算每一层参数的偏导数，并更新参数。（2）梯度消失问题根据误差反向传播的迭代公式�0�2<span class="g-font-color green">误差从输出层反向传播时，在每一层都要乘以该层的激活函数的导数。</span>而对于logistic函数和tanh函数，它们的导数值域都小于1.�0�2�0                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（互联网）：</p>
                            <div class="local-source-detail">
                                <b>标题：</b>深度学习总结笔记（二） - zgyggy的博客 - CSDN博客<br/><a href='https://blog.csdn.net/zgyggy/article/details/78363713?utm_source=blogxgwz8' target='_blank'>https://blog.csdn.net/zgyggy/article/details/78363713?utm_source=blogxgwz8</a>
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="互联网">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">2</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color red similarNum">100%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color red"><span class="g-underline-text">误差</span><span class="g-underline-text">从</span><span class="g-underline-text">输出</span><span class="g-underline-text">层</span><span class="g-underline-text">反向</span><span class="g-underline-text">传播</span><span class="g-underline-text">时</span>，<span class="g-underline-text">在</span><span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">误差</span><span class="g-underline-text">从</span><span class="g-underline-text">输出</span><span class="g-underline-text">层</span><span class="g-underline-text">反向</span><span class="g-underline-text">传播</span><span class="g-underline-text">时</span>，<span class="g-underline-text">在</span><span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            分为以下三步：前馈计算每一层的净输入和激活值，直到最后一层；反向传播计算每一层的误差项；计算每一层参数的偏导数，并更新参数。<span class="g-font-color green">误差从输出层反向传播时，在每一层都要乘以该层的激活函数的导数。</span>我们可以看到，sigmoid型函数导数的值域都小于1。并且由于sigmoid型函数的饱和性                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（互联网）：</p>
                            <div class="local-source-detail">
                                <b>标题：</b> 深度学习最精炼中文讲义 前馈与卷积神经网络详解，复旦邱锡鹏老师《神经网络与深度学习》报告分享02（附报告pdf下载） - 云+社区 - 腾讯云<br/><a href='https://cloud.tencent.com/developer/article/1089966' target='_blank'>https://cloud.tencent.com/developer/article/1089966</a>
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="互联网">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">3</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color red similarNum">97%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color red"><span class="g-underline-text">误差</span><span class="g-underline-text">从</span><span class="g-underline-text">输出</span><span class="g-underline-text">层</span><span class="g-underline-text">反向</span><span class="g-underline-text">传播</span><span class="g-underline-text">时</span>，<span class="g-underline-text">在</span><span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">误差</span><span class="g-underline-text">从</span><span class="g-underline-text">输出</span><span class="g-underline-text">层</span><span class="g-underline-text">反向</span><span class="g-underline-text">传播</span><span class="g-underline-text">时</span>，<span class="g-underline-text">在</span><span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>，</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            <span class="g-font-color green">误差从输出层反向传播时，在每一层都要乘以该层的激活函数的导数，</span>如果导数的值域小于1，甚至如Sigmoid函数在两端的饱和区导数趋于0，那么                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（互联网）：</p>
                            <div class="local-source-detail">
                                <b>标题：</b>深度学习之前馈神经网络(前向传播和误差方向传播)<br/><a href='https://www.so.com/link?m=aFoqGum2%2BrZT9I79dRv%2Fle10CpYeNBFKD8tpCCMx7v60hVQUj0PPRZOXl5QTc9VtvzE%2F5tokZj8wu4R23netxWey1OeDEGPZE4MHANFZ8ykhS8i9AoZcXGDctMoWDM5nXf14T0C6%2BZPjbki9VS4ZD8IbxOZZLErTpgqPz7a3%2B%2BZr4hKEsYPbhk1SqkQRVwrnerBI731VsyLt%2FYQeKQn7Cq37k0CE1KO8vymQ%2F3mG9kh5Izn5eCWoqX5AE2y9QEFHq6wsvPACVRp852eUAIsylL%2BTSLrkqA7QTwj0zDpVoDgpYjYHmAjaMBNZP0OmjHKPBUjKuF0tP0hArnxhxXd20gUkbZSDKQYI%2BV0bHEQD8uQlOVQawjbYQTyQ6p0s5tckbNAF9SFbBAIo1dExwIlS0mMaO4zmfWSkOrYzIgqsz5RSjpuZkuweqBZ0Wkv0%2FUvJjGkvhn4BPv2CEuHKZHa4FvQbMT0tNT6yDrwvG4Y4lWQB5T0%2F1iET%2FJA%3D%3D' target='_blank'>https://www.so.com/link?m=aFoqGum2%2BrZT9I79dRv%2Fle10CpYeNBFKD8tpCCMx7v60hVQUj0PPRZOXl5QTc9VtvzE%2F5tokZj8wu4R23netxWey1OeDEGPZE4MHANFZ8ykhS8i9AoZcXGDctMoWDM5nXf14T0C6%2BZPjbki9VS4ZD8IbxOZZLErTpgqPz7a3%2B%2BZr4hKEsYPbhk1SqkQRVwrnerBI731VsyLt%2FYQeKQn7Cq37k0CE1KO8vymQ%2F3mG9kh5Izn5eCWoqX5AE2y9QEFHq6wsvPACVRp852eUAIsylL%2BTSLrkqA7QTwj0zDpVoDgpYjYHmAjaMBNZP0OmjHKPBUjKuF0tP0hArnxhxXd20gUkbZSDKQYI%2BV0bHEQD8uQlOVQawjbYQTyQ6p0s5tckbNAF9SFbBAIo1dExwIlS0mMaO4zmfWSkOrYzIgqsz5RSjpuZkuweqBZ0Wkv0%2FUvJjGkvhn4BPv2CEuHKZHa4FvQbMT0tNT6yDrwvG4Y4lWQB5T0%2F1iET%2FJA%3D%3D</a>
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">4</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color red similarNum">94%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color red"><span class="g-underline-text">误差</span><span class="g-underline-text">从</span><span class="g-underline-text">输出</span><span class="g-underline-text">层</span><span class="g-underline-text">反向</span><span class="g-underline-text">传播</span><span class="g-underline-text">时</span>，<span class="g-underline-text">在</span><span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">误差</span><span class="g-underline-text">从</span><span class="g-underline-text">输出</span><span class="g-underline-text">层</span><span class="g-underline-text">反向</span><span class="g-underline-text">传播</span><span class="g-underline-text">时</span>，<span class="g-underline-text">在</span><span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。当</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            一层参数的导数；9（4）更新参数；2.1.5梯度消失问题里，代表向量的点积运算符，f'表示激活函数的导数。因此<span class="g-font-color green">误差从输出层反向传播时，在每一层都要乘以该层激活函数的导数。当</span>使用sigmoid或tanh作为激活函数时，其导数为：从公式中可以看出，sigmoid函数                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于卷积神经网络的行人性别识别方法研究》<br><b>作者：</b>蔡磊<br><b>学科专业：</b>信息与通信工程<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>曾焕强<br><b>学位授予单位：</b>华侨大学<br><b>学位年度：</b>2018
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="互联网">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">5</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color red similarNum">70%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color red"><span class="g-underline-text">误差</span>从输出<span class="g-underline-text">层</span><span class="g-underline-text">反向</span><span class="g-underline-text">传播</span>时，在<span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">误差</span><span class="g-underline-text">反向</span><span class="g-underline-text">传播</span><span class="g-underline-text">的</span>迭代过程中，<span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            利用梯度下降法更新参数），不断循环此过程，直到神经网络模型在验证集v上的错误率不再下降。神经网络的优化问题是一个非凸优化问题。在用<span class="g-font-color green">误差反向传播的迭代过程中，每一层都要乘以该层的激活函数的导数。</span>当我们使用sigimod型函数时，他们导数的值域都小于或者等于1。由于sigimod函数的                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（互联网）：</p>
                            <div class="local-source-detail">
                                <b>标题：</b>神经网络及反向传播算法_神经网络，网络，算法_Mikow的博客-CSDN博客<br/><a href='https://blog.csdn.net/Mikow/article/details/106172805' target='_blank'>https://blog.csdn.net/Mikow/article/details/106172805</a>
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">6</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">62%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange">误差从输出<span class="g-underline-text">层</span>反向<span class="g-underline-text">传播</span>时，<span class="g-underline-text">在</span><span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span>激活<span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">传播</span>，上述公式也不难看出<span class="g-underline-text">在</span><span class="g-underline-text">每</span><span class="g-underline-text">一<span class="g-underline-text">层</span></span><span class="g-underline-text">都要</span><span class="g-underline-text">乘以</span><span class="g-underline-text">该</span><span class="g-underline-text">层</span><span class="g-underline-text">的</span>激励<span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。当然</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            h1’/}of’f，zⅣ’/)，这其中/’¨表示激励函数的导数。从反向传播算法的原理可以知道，网络的误差从输出层反向<span class="g-font-color green">传播，上述公式也不难看出在每一层都要乘以该层的激励函数的导数。当然</span>，一般所使用的激励函数为sigmoid函数和ta行h函数，那么其导数分别是：                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于深度卷积神经网络的人脸识别研究》<br><b>作者：</b>杨子文<br><b>学科专业：</b>电子与通信工程<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>曾上游<br><b>学位授予单位：</b>广西师范大学<br><b>学位年度：</b>2017
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">7</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">44%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange">误差从输出<span class="g-underline-text">层</span>反向传播时，在每一<span class="g-underline-text">层</span>都要<span class="g-underline-text">乘以</span>该<span class="g-underline-text">层</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green">1<span class="g-underline-text">层</span><span class="g-underline-text">的</span>节点<span class="g-underline-text">的</span>权值。接着，<span class="g-underline-text">乘以</span>当前<span class="g-underline-text">层</span>预输入u<span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            看出，为了求第l层的神经节点的灵敏度，应当首先对和当前第l层相连接的对应的第l+1层的灵敏度求和，再乘以第l+<span class="g-font-color green">1层的节点的权值。接着，乘以当前层预输入u的激活函数的导数。</span>在一个卷积层紧接着一盒下采样层的情况下，下一层的一个像素的关联灵敏                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于深度学习和空间邻域信息的极化SAR地物分类方法研究》<br><b>作者：</b>樊伟明<br><b>学科专业：</b>电子与通信工程<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>岳博<br><b>学位授予单位：</b>西安电子科技大学<br><b>学位年度：</b>2017
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学术期刊">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">8</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">41%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange">误差从输出层<span class="g-underline-text">反向</span><span class="g-underline-text">传播</span>时，<span class="g-underline-text">在</span>每一层都要乘以该层<span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green">!他仍采用<span class="g-underline-text">反向</span><span class="g-underline-text">传播</span><span class="g-underline-text">的</span>思想!<span class="g-underline-text">在</span><span class="g-underline-text">反向</span>推导中用调整<span class="g-underline-text">函数</span>优化<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>"</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            算法改进万方数据殢㎠图 -;=P算法与?-;=P算法的函数逼近性能比较#"结语本文提出一种新的改进=P算法-;=P算法<span class="g-font-color green">!他仍采用反向传播的思想!在反向推导中用调整函数优化激活函数的导数"</span>调整函数的作用在于简化激活函数的求导运算#优化梯度!具体选用的函数形式视问题                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学术期刊）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《采用调整函数优化梯度的BP算法改进》<br><b>作者：</b>梅冬芳<br><b>作者单位：</b>华南理工大学,政治与公共管理学院,广东,广州,510641<br><b>参考文献：</b>8篇<br><b>被引次数：</b>1次（统计时间：2015年8月）<br><b>页码：</b>P102—P105<br><b>页数：</b>4页<br><b>分类号：</b>TP301.6<br><b>机标分类号：</b>TP3 O1<br><b>期刊名称：</b>《现代电子技术》<br><b>出版时间：</b>2006年16期<br><b>期刊级别：</b>ISTIC<br><b>ISSN：</b>1004-373X<br><b>关键词：</b>调整函数 BP算法 激活函数 导数<br><b>摘要：</b>针对基于梯度下降法的BP算法提出一种改进方法:AF法.即在权值修正中,引入一个调整函数优化梯度.调整函数作用在激活函数的导数上,用于平缓和优化激活函数的导数运算.并就4-元校验码和Feigenbaum函数逼近问题进行算法比较.实验显示,应用本方法的单隐层BP网就能迅速有效地解决异或问题和4-元校验码问题.
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">9</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">41%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange">误差从输出层反向传播时，在每一层都要<span class="g-underline-text">乘以</span><span class="g-underline-text">该</span>层<span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">的</span>所有神经元<span class="g-underline-text">的</span>残差加权求和，然后再<span class="g-underline-text">乘以</span><span class="g-underline-text">该</span>神经元<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span><span class="g-underline-text">导数</span>。</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            差是有关的，即第m层卷积层的特征图中的某一个神经元的残差应该等于第m1层的特征图中与该神经元有连接关系<span class="g-font-color green">的所有神经元的残差加权求和，然后再乘以该神经元激活函数的导数。</span>然而，CNN的结构往往在卷积层后接上一个下采样层，即池化层。这样池化层的                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于深度网络的SAR图像目标检测技术研究》<br><b>作者：</b>肖定坤<br><b>学科专业：</b>电子与通信工程<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>晋本周<br><b>学位授予单位：</b>西安电子科技大学<br><b>学位年度：</b>2017
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
            
                <!--语句修改建议-->
        <span id="modify_suggest"></span>
        <div id="advice">
            <div class="g-line-row"></div>
            <div class="paper-txt P30 PB0">
                <div class="paper-section">
                    <p class="g-font-s16 font-bold g-font-color green MB10">该句修改建议（重度相似，请全面修改）</p>
                    <span class="g-font-color green"><span class="g-font-color red">误差</span>从<span class="g-font-color red">输出</span><span class="g-font-color red">层</span><span class="g-font-color red">反向</span><span class="g-font-color red">传播</span><span class="g-font-color red">时</span>，在每<span class="g-font-color red">一<span class="g-font-color red">层</span></span><span class="g-font-color red">都要</span><span class="g-font-color red">乘以</span>该<span class="g-font-color red">层</span>的<span class="g-font-color red">激活</span><span class="g-font-color red">函数</span>的<span class="g-font-color red">导数</span>。</span>
                    <div class="MT10">
                        <p class="font-bold">同义词：</p>
                        <ul class="local-source-detail">
                            <li class="g-font-color green"><span class="g-font-color red">输出：</span>出口</li><li class="g-font-color green"><span class="g-font-color red">函数：</span>因变量</li>
                        </ul>

                    </div>
                </div>
            </div>
        </div>
            </div>
    <div class="back-to-top text-center">
        <a href="#gototop" class="font-bold g-font-color green">回到顶部</a>
    </div>
    <div class="paper-footer">
        <p>检测报告由<a href="http://www.paperpass.com/" target="_black">PaperPass</a>文献相似度检测系统生成</p>
        <p>Copyright © 2007-2020 PaperPass</p>
    </div>
</div>
</body>
<script type="text/javascript" src="../js/jquery.min.js"></script>
<script type="text/javascript" src="../js/Lib.js"></script>
<script type="text/javascript">
    (function(System,$){
        var tab = System.Paper.tab();
        var $advice = null;
        function run(){
            var i = 0;
            $advice.show();
            tab.call(this,{callback:function(){
                var $num = $(this).find('.chapter-num');
                if($num.length>0){
                    i++;
                    $num.text(i);
                }
            }});
            //没有内容时同时不显示该句修改建议
            if(0 === i){$advice.hide();}
        }
        $(function(){
            $advice = $('#advice');
            $(document).on('click','[tab-a="ul"] li',function(){
                run.call(this);
            });
            $(document).on('click','[tab-b="ul"] li',function(){
                run.call(this);
            });
        });
    })(Report,jQuery);


</script>
</html>
