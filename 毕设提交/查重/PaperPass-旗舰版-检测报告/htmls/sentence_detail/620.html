<!DOCTYPE html>
<html>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta name="keywords" content="">
    <meta name="description" content="">
    <title>详细报告语句详情</title>
    <link href="../css/bootstrap.min.css" rel="stylesheet" />
    <link href="../css/style.css" rel="stylesheet" />
    <style type="text/css">
        .content-tips .paper-section{margin-bottom:0;min-width:450px;}
    </style>
</head>
<body>
<span id="gototop"></span>
<div>
    <div class="paper-txt P30">
        <div class="paper-section" style="margin-bottom:0;">
            <div class="font-bold MT5">
                该句相似度：<span class="g-font-color orange similarNum">55%</span>
            </div>
            <div class="MT30">
                <p class="font-bold">您的语句：</p>
                <span>常用的非线性激活函数有Sigmoid、ReLU等，原始的GCN采用ReLU作为激活函数。</span>
            </div>
        </div>
    </div>
    <div>
        <div class="tab-A">
            <ul class="tab-A-ul clearfix" tab-a="ul">
                                        <li class="active" data-id="学术期刊,学位论文,学术会议,书籍数据,互联网,自建库,all">
                            <span class="tab-text">综合</span>
                            <span class="tab-superscript">14</span>
                        </li>
                                    <li data-id="tab,学术期刊,学位论文,学术会议,书籍数据,自建库,local"
                                    >
                    <span class="tab-text">本地库</span>
                    <span class="tab-superscript">14</span>
                </li>
                                <li data-id="互联网,5">
                    <span class="tab-text">互联网</span>
                    <span class="tab-superscript">0</span>
                </li>
                            </ul>
        </div>
        <div class="none" tab="section" data-id="tab"
                    >
            <div class="tab-B clearfix">
                <div class="paper-section P30 PB0 clearfix" style="padding:30px 10px 0">
                <ul class="tab-B-ul pull-left clearfix" tab-b="ul">
                        <li class="active" data-id="tab,学术期刊,学位论文,学术会议,书籍数据,自建库,local">
                            <span class="tab-text">全部</span>
                            <span class="tab-superscript">14</span>
                            <span class="black-spacer"></span>
                        </li>
                        <li data-id="tab,学术期刊,1">
                            <span class="tab-text">期刊</span>
                            <span class="tab-superscript">2</span>
                            <span class="black-spacer"></span>
                        </li>
                        <li data-id="tab,学位论文,2">
                            <span class="tab-text">学位</span>
                            <span class="tab-superscript">11</span>
                            <span class="black-spacer"></span>
                        </li>
                        <li data-id="tab,学术会议,3">
                            <span class="tab-text">会议</span>
                            <span class="tab-superscript">0</span>
                            <span class="black-spacer"></span>
                        </li>
                        <li data-id="tab,书籍数据,4">
                            <span class="tab-text">图书</span>
                            <span class="tab-superscript">1</span>
                                                    </li>
                                            </ul>
                </div>
                <div class="g-line-row MT30 tab-pane-line"></div>
            </div>
        </div>
        <div class="content-tips">
            <div class="tab-content none" tab="section" data-id="1">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在期刊库共找出相似内容：<span>2</span>个                    <p>
                                        <div style="margin-top: 10px;">
                    <a href="#modify_suggest" class="g-btn g-btn-default g-btn-sm">查看修改意见</a>
                    </div>
                                    </div>
            </div>

            <div class="tab-content none" tab="section" data-id="2">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在学位库共找出相似内容：<span>11</span>个                    <p>
                                        <div style="margin-top: 10px;">
                    <a href="#modify_suggest" class="g-btn g-btn-default g-btn-sm">查看修改意见</a>                    </div>
                                    </div>
            </div>
            <div class="tab-content none" tab="section" data-id="3">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在会议库中没有找到与此句话相似的内容                    <p>
                                        <div style="margin-top: 10px;">
                                        </div>
                                    </div>
            </div>
            <div class="tab-content none" tab="section" data-id="4">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在图书库共找出相似内容：<span>1</span>个                    <p>
                                        <div style="margin-top: 10px;">
                    <a href="#modify_suggest" class="g-btn g-btn-default g-btn-sm">查看修改意见</a>                    </div>
                                    </div>
            </div>
            <div class="tab-content none" tab="section" data-id="5">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在互联网库中没有找到与此句话相似的内容                    <p>
                                        <div style="margin-top: 10px;">
                                        </div>
                                    </div>
            </div>
            <!-- 自建库 -->
            <div class="tab-content none" tab="section" data-id="6">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在自建库中没有找到与此句话相似的内容                    <p>
                                        <div style="margin-top: 10px;">
                                        </div>
                                    </div>
            </div>
                        <div class="tab-content" tab="section" data-id="all">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                                                在本地库和互联网库共找出相似内容：<span>14</span>个
                                            <p>
                                            <div style="margin-top: 10px;">
                    <a href="#modify_suggest" class="g-btn g-btn-default g-btn-sm">查看修改意见</a>

                    </div>
                                    </div>
            </div>
            <div class="tab-content none" tab="section" data-id="local">
                <div class="paper-txt P30 paper-section clearfix">
                    <p class="g-font-color green">
                        在本地库共找出相似内容：<span>14</span>个                    <p>
                                        <div style="margin-top: 10px;">
                    <a href="#modify_suggest" class="g-btn g-btn-default g-btn-sm">查看修改意见</a>
                    </div>
                                    </div>
            </div>
        </div>



                <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">1</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">55%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span>非线性<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid、ReLU等，原始<span class="g-underline-text">的</span>GCN采用ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid<span class="g-underline-text">函数</span>、Tanh<span class="g-underline-text">函数</span>和ReLU<span class="g-underline-text">函数</span>。Sigmoid<span class="g-underline-text">函数</span>也称为Logistic<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span></p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            。这一条件是以梯度为基础的优化方法的前提。（3）输出值的范围有限。当激活函数的输出范围有限时，基于梯度的训练方法才更加稳定。<span class="g-font-color green">常用的激活函数有Sigmoid函数、Tanh函数和ReLU函数。Sigmoid函数也称为Logistic激活函数</span>，其将实数值压缩到0和1之间，将大的负数映射为0，大的                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于迁移学习的旅游景点个性化推荐算法》<br><b>作者：</b>韩国锋<br><b>学科专业：</b>控制科学与工程<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>郑恩让<br><b>学位授予单位：</b>陕西科技大学<br><b>学位年度：</b>2019
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">2</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">55%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span><span class="g-underline-text">非</span><span class="g-underline-text">线性</span>激活<span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid、ReLU等，原始<span class="g-underline-text">的</span>GCN采用ReLU作为激活<span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span><span class="g-underline-text">非</span><span class="g-underline-text">线性</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid<span class="g-underline-text">非</span><span class="g-underline-text">线性</span><span class="g-underline-text">函数</span><span class="g-underline-text">的</span>特点是：在信号<span class="g-underline-text">的</span>两侧边缘部分，</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            ）非线性激活函数线性函数只适用于简单的线性分类，而在实际应用中，还有一大部分是非线性分类和识别任务，因此需要更加复杂的非线性函数，最<span class="g-font-color green">常用的非线性函数有Sigmoid非线性函数的特点是：在信号的两侧边缘部分，</span>信号的变化率增益较小，但是在中间部分，信号得到的变化增益较大，所以在不同                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于卷积神经网络的视频中的人体行为识别研究》<br><b>作者：</b>姜枫<br><b>学科专业：</b>物理电子学<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>张丽红<br><b>学位授予单位：</b>山西大学<br><b>学位年度：</b>2016
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学术期刊">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">3</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">52%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span>非线性<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid、ReLU<span class="g-underline-text">等</span>，原始<span class="g-underline-text">的</span>GCN采用ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green">，<span class="g-underline-text">常用</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>sigmoid<span class="g-underline-text">函数</span>、高斯<span class="g-underline-text">函数</span>、双曲正切<span class="g-underline-text">函数</span><span class="g-underline-text">等</span>。尽管这些<span class="g-underline-text">函数</span>在</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            性能而广泛应用于非线性系统的建模El3。在使用神经网络解决给定的实际逼近问题时，激活函数的选择对神经网络的结构和逼近性能有很大影响<span class="g-font-color green">，常用的激活函数有sigmoid函数、高斯函数、双曲正切函数等。尽管这些函数在</span>实际建模过程中已得到了广泛应用，但也存在一些缺点，如过训练，易                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学术期刊）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于正交最小二乘的傅立叶神经网络结构选取方法》<br><b>作者：</b>段超霞 田学民<br><b>作者单位：</b>国家知识产权局专利局专利审查协作广东中心,广州,510530；中国石油大学信息与控制工程学院,山东青岛,266580<br><b>参考文献：</b>11篇<br><b>被引次数：</b>1次（统计时间：2015年8月）<br><b>页码：</b>P19—P23<br><b>页数：</b>5页<br><b>分类号：</b>TP273<br><b>机标分类号：</b>O23 TN<br><b>基金项目：</b>国家自然科学基金资助(51104175);山东省自然科学基金资助(ZR2011FM014)<br><b>期刊名称：</b>《石油化工自动化》<br><b>出版时间：</b>2012年6期<br><b>ISSN：</b>1007-7324<br><b>关键词：</b>傅立叶神经网络正交最小二乘算法激活函数<br><b>摘要：</b>针对傅立叶神经网络基频选取及隐含层神经元数目难以确定这一问题，提出一种基于正交最小二乘的傅立叶神经网络结构选择方法。该方法依据傅立叶神经网络激活函数是一系列不同函数的特点，给定一个较小的基频，把一系列关于基频整数倍的正余弦函数作为可供选择的激活函数集，然后由正交最小二乘算法选择对网络性能贡献显著的函数作为激活函数，并简化网络结构。以聚合反应器为对象的建模仿真研究表明，正交最小二乘算法可以更有效地确定傅立叶神经网络结构。
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">4</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">50%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange">常用<span class="g-underline-text">的</span>非线性<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid、ReLU<span class="g-underline-text">等</span>，原始<span class="g-underline-text">的</span>GCN采用ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green">0。一常见<span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>sigmoid<span class="g-underline-text">函数</span>，tanh<span class="g-underline-text">函数</span>，relu<span class="g-underline-text">函数</span><span class="g-underline-text">等</span>，下图4-</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            ．激活函数输出必须为[0，1]之间的值；2．函数在充分活跃时，输出值将为1，而当该神经元被抑制时，输出值将为<span class="g-font-color green">0。一常见的激活函数有sigmoid函数，tanh函数，relu函数等，下图4-</span>2为这三种激活函数图像。图4．2：三种常见激活函数图像sigmoid函数                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于深度学习的有害气体红外图像处理研究》<br><b>作者：</b>林云<br><b>学科专业：</b>信息与通信工程<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>王效灵<br><b>学位授予单位：</b>浙江工商大学<br><b>学位年度：</b>2017
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">5</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">50%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span>非线性<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid、ReLU<span class="g-underline-text">等</span>，原始<span class="g-underline-text">的</span>GCN采用ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>S曲线<span class="g-underline-text">函数</span>Sigmoid、双曲<span class="g-underline-text">函数</span>tanh、ReLU<span class="g-underline-text">等</span>。由前文可知</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            器件特点进行算法设计与优化，尤其是对于计算较为复杂的Sigmoid/tanh函数。万方数据第3章全连接神经网络的算法实现与优化3．7激活函数的实现与优化<span class="g-font-color green">常用的激活函数有S曲线函数Sigmoid、双曲函数tanh、ReLU等。由前文可知</span>，ReLU的算法实现比较简单，因此无需特别优化；tanh函数可以由Sigmoid函数经过简单的                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《全连接神经网络在FPGA上的实现与优化》<br><b>作者：</b>周鑫<br><b>学科专业：</b>计算机系统结构<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>安虹<br><b>学位授予单位：</b>中国科学技术大学<br><b>学位年度：</b>2018
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">6</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">49%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span><span class="g-underline-text">非</span><span class="g-underline-text">线性</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid、ReLU等，原始<span class="g-underline-text">的</span>GCN采用ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。<span class="g-underline-text">常用</span><span class="g-underline-text">的</span>饱和<span class="g-underline-text">非</span><span class="g-underline-text">线性</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid<span class="g-underline-text">函数</span>[391和双曲正切<span class="g-underline-text">函数</span>Tard[</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            /u图3-1近似生物神经的激活函数原理3．1常见的几种激活函数CNN中的激活函数主要分为两种：饱和非线性激活函数以及不饱和非线性<span class="g-font-color green">激活函数。常用的饱和非线性激活函数有Sigmoid函数[391和双曲正切函数Tard[</span>1C40]；常用的不饱和非线性激活函数主要有ReLU函数‘411、LReLu                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于改进卷积神经网络的图像分类研究》<br><b>作者：</b>曹惠珍<br><b>学科专业：</b>计算机科学与技术<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>周生明<br><b>学位授予单位：</b>广西师范大学<br><b>学位年度：</b>2017
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">7</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">49%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange">常用的非线性<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>有Sigmoid、ReLU等，原始的GCN采用ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green">、Gauss<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>、Softplus<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>、ReLU<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>、Mix-Uniform<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>以及Mix-</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            心电图导联重建实验和结果分析回归问题也是人工神经网络应用的一个重要方面。本文以心电图导联重建作为拟合应用的例子，来分析Sigmoid激活函数、Tanh激活函数<span class="g-font-color green">、Gauss激活函数、Softplus激活函数、ReLU激活函数、Mix-Uniform激活函数以及Mix-</span>GA激活函数这7种激活函数的性能。心电图是心脏检测非常重要的一种手段，                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《蒙特卡洛方法神经网络的激活函数研究及应用》<br><b>作者：</b>徐梓荐<br><b>学科专业：</b>软件工程<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>周庆国<br><b>学位授予单位：</b>兰州大学<br><b>学位年度：</b>2018
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">8</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">49%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span>非线性<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>有Sigmoid、ReLU等，原始<span class="g-underline-text">的</span>GCN采用ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span>四种<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>Sigmoid<span class="g-underline-text">函数</span>、双曲正切<span class="g-underline-text">函数</span>Tanh、ReLUs<span class="g-underline-text">函数</span>以及Softplus<span class="g-underline-text">函数</span>下<span class="g-underline-text">的</span></p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            以验证本章算法。实验主要工作有以下两个方面：（1）基于传统的LeNet-5卷积神经网络算法（CNN），比较卷积神经网络采用改进后的激活函数以及采用<span class="g-font-color green">常用的四种激活函数Sigmoid函数、双曲正切函数Tanh、ReLUs函数以及Softplus函数下的</span>识别效果；（2）为了比较本章4.4小节所提两种结构的识别效果，                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于改进卷积神经网络算法的研究与应用》<br><b>作者：</b>王飞飞<br><b>分类号：</b>TP391.41<br><b>学科专业：</b>软件工程<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>荆晓远<br><b>学位授予单位：</b>南京邮电大学<br><b>学位年度：</b>2016<br><b>关键词：</b>图像识别 卷积神经网络 加权Fisher准则 激活函数 Gabor滤波器
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">9</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">48%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span>非线性<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid、ReLU等，原始<span class="g-underline-text">的</span>GCN采用ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green">。<span class="g-underline-text">常用</span><span class="g-underline-text">的</span>饱和<span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>主要<span class="g-underline-text">有</span>Sigmoid和Tanh。常见<span class="g-underline-text">的</span>不饱和<span class="g-underline-text">的</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>主要</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            效果很差[45]。所以选择合适激活函数十分重要。图3-1激活函数卷积神经网络中常用的激活函数可以分为饱和的激活函数和不饱和的激活函数<span class="g-font-color green">。常用的饱和的激活函数主要有Sigmoid和Tanh。常见的不饱和的激活函数主要</span>万方数据Ⳃ瘒吀耀华中科技大学硕士学位论文Sigmoid函数是一个在生物学中常见的                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于卷积神经网络的冷轧薄板表面缺陷分类算法研究》<br><b>作者：</b>王孟嬉<br><b>学科专业：</b>机械电子工程<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>陈幼平<br><b>学位授予单位：</b>华中科技大学<br><b>学位年度：</b>2017
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学术期刊">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">10</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">48%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange">常用的非<span class="g-underline-text">线性</span>激活<span class="g-underline-text">函数</span>有Sigmoid、ReLU等，原始的GCN采用ReLU作为激活<span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green">出现了很多调整变化，例如sigmoid<span class="g-underline-text">函数</span>、Relu<span class="g-underline-text">函数</span>、LeakyRelu<span class="g-underline-text">函数</span>、ssatu<span class="g-underline-text">函数</span>，<span class="g-underline-text">线性</span></p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            对损失函数根据复合函数求导法则进行求导，迭代更新模型参数W:其中，Wl表示第l层模型参数。2激活函数随着深度学习的深入研究，激活函数<span class="g-font-color green">出现了很多调整变化，例如sigmoid函数、Relu函数、LeakyRelu函数、ssatu函数，线性</span>激活函数的导数性质作用于CNN算法的参数更新，从而影响其模型训练的收敛性。                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学术期刊）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于CNN的车辆检测中激活函数的研究》<br><b>作者：</b>周必书 黄立勤<br><b>作者单位：</b>福州大学 物理与信息工程学院,福建 福州,350116<br><b>参考文献：</b>23篇<br><b>页码：</b>P76—P82<br><b>页数：</b>7页<br><b>分类号：</b>TP391.7<br><b>基金项目：</b>国家自然科学基金项目资助(61471124);福建省科技重大项目资助(2017H6009);赛尔网络创新项目资助(NGII20160208, NGII20170201)<br><b>期刊名称：</b>《贵州大学学报（自然科学版）》<br><b>出版时间：</b>2018年6期<br><b>期刊级别：</b>ISTIC<br><b>ISSN：</b>1000-5269<br><b>关键词：</b>卷积神经网络 车辆检测 激活函数<br><b>摘要：</b>视频车辆检测是计算机视觉应用于汽车辅助驾驶系统的主要技术难点之一,卷积神经网络是现在视频车辆检测性能最好的计算机视觉算法,激活函数是卷积神经网络算法的重要模块,影响神经网络的收敛性和精确度.本文主要在模型训练和模型验证两个阶段分析激活函数的影响,讨论的函数包括sigmoid函数、Relu函数、Leaky-Relu函数及提出的一种半饱和ssatu函数,实验是以车辆检测YOLOv2算法为基础对不同激活函数的效果做了比较分析.实验表明:软饱和函数sigmoid和函数ssatu使模型收敛的速度最快,且连续可导非线性sigmoid函数使模型训练中损失值的振荡最小;在模型性能上体现出不抑制特征点的分段函数更适用于一般性的创建卷积神经网络.
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="书籍数据">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">11</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">47%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange">常用<span class="g-underline-text">的</span><span class="g-underline-text">非</span><span class="g-underline-text">线性</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>有Sigmoid、ReLU等，原始<span class="g-underline-text">的</span>GCN<span class="g-underline-text">采用</span>ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green">，如果<span class="g-underline-text">非</span><span class="g-underline-text">线性</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>如Sigmoid<span class="g-underline-text">函数</span>被<span class="g-underline-text">采用</span>，或其他<span class="g-underline-text">的</span>误差<span class="g-underline-text">函数</span>用于确定参数，</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            证明，对于一个线性网络，极小化平方和误差函数的权重可解析地给出，但值得注意的是这一结果仅当误差函数为平方和且网络为线性时才能成立<span class="g-font-color green">，如果非线性激活函数如Sigmoid函数被采用，或其他的误差函数用于确定参数，</span>那么，问题的解便不可能直接给出．然而，如果激活函数是一光滑函数，                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（书籍数据）：</p>
                            <div class="local-source-detail">
                                <b>章节：</b>《第二章 仿生结构算法：人工神经网络               115页》<br><b>书名：</b>《计算智能中的仿生学》<br><b>作者：</b>徐宗本<br><b>出版社：</b>科学出版社<br><b>出版时间：</b>2003-5-1<br><b>ISBN：</b>7-03-010792-6
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">12</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">46%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange"><span class="g-underline-text">常用</span><span class="g-underline-text">的</span><span class="g-underline-text">非</span><span class="g-underline-text">线性</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>有Sigmoid、ReLU<span class="g-underline-text">等</span>，原始<span class="g-underline-text">的</span>GCN采用ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green">这个问题，但<span class="g-underline-text">常用</span><span class="g-underline-text">的</span><span class="g-underline-text">非</span><span class="g-underline-text">线性</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>如sigmoid、tanh<span class="g-underline-text">等</span>不具备稀疏性并且容易</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            DNN还要高出一到两个数量级，因此像文献[31】中多达几十上百层的深度对于RNN来说是非常难以训练的。使用非线性的激活函数可以较好的解决<span class="g-font-color green">这个问题，但常用的非线性激活函数如sigmoid、tanh等不具备稀疏性并且容易</span>陷入梯度消失问题中。本文结合针对以上问题，提出了两个改进激活函数的方案：s                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于RNN的发电机组排放预测及发电调度研究》<br><b>作者：</b>杨训政<br><b>分类号：</b>TM301<br><b>学科专业：</b>信息安全<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>熊焰<br><b>学位授予单位：</b>中国科学技术大学<br><b>学位年度：</b>2016<br><b>关键词：</b>发电机组 排放预测 发电调度 递归神经网络
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">13</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">45%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange">常用的非线性<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>有Sigmoid、ReLU等，原始的GCN采用ReLU<span class="g-underline-text">作为</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green">使用Tanh<span class="g-underline-text">函数</span><span class="g-underline-text">作为</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>平均耗时也比ReLU多，从而说明了ReLU<span class="g-underline-text">作为</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>时</p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            层的消耗时间明显缩短.这样，使用Sigmoid作为激活函数训练自编码迭代100次平均耗时为245.86秒，而ReLU作为激活函数仅需要127.83秒，<span class="g-font-color green">使用Tanh函数作为激活函数平均耗时也比ReLU多，从而说明了ReLU作为激活函数时</span>网络收敛速度之快.无论从表2还是图7中，上述实验结果都说明使用ReLU                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于改进的SAE和稀疏滤波算法的文本分类研究》<br><b>作者：</b>崔嘉乐<br><b>分类号：</b>TP391.1<br><b>学科专业：</b>应用数学<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>裴志利<br><b>学位授予单位：</b>内蒙古民族大学<br><b>学位年度：</b>2017<br><b>关键词：</b>文本分类 稀疏滤波 SAE算法 特征学习
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
                    <div class="tab-content" tab="section" data-id="学位论文">
            <div>
                <div class="g-line-row"></div>
                <div class="paper-txt P30">
                    <div class="paper-section" style="margin-bottom:0;">
                        <div class="clearfix">
                            <div class="pull-left">
                                <span class="chapter-symbol"></span>
                                <span class="chapter-num">14</span>

                            </div>
                            <div class="font-bold pull-right">
                                相似度：<span class="g-font-color orange similarNum">45%</span>
                            </div>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">您的句子：</p>
                            <p class="g-font-color orange">常用<span class="g-underline-text">的</span><span class="g-underline-text">非</span><span class="g-underline-text">线性</span><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>Sigmoid、ReLU<span class="g-underline-text">等</span>，原始<span class="g-underline-text">的</span>GCN采用ReLU作为<span class="g-underline-text">激活</span><span class="g-underline-text">函数</span>。</p>

                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似句子：</p>
                            <p class="g-font-color green"><span class="g-underline-text">激活</span><span class="g-underline-text">函数</span><span class="g-underline-text">有</span>双曲正切<span class="g-underline-text">函数</span>和Sigmoid<span class="g-underline-text">函数</span><span class="g-underline-text">等</span>饱和<span class="g-underline-text">非</span><span class="g-underline-text">线性</span><span class="g-underline-text">函数</span>，然而使用此类<span class="g-underline-text">函数</span><span class="g-underline-text">的</span></p>
                        </div>
                        <div class="MT10">
                            <p class="font-bold">相似原文片段：</p>
                            的CNNs图像识别方法3.1引言在卷积神经网络中，输入图像经过卷积阶段得到的特征需要经过非线性映射的方式来避免线性操作表达能力不足的问题，常用的<span class="g-font-color green">激活函数有双曲正切函数和Sigmoid函数等饱和非线性函数，然而使用此类函数的</span>网络收敛很慢或者由于梯度弥散导致不收敛[29]，尤其是双曲正切曲线存在负                        </div>
                        <div class="MT10">
                            <p class="font-bold">来源（学位论文）：</p>
                            <div class="local-source-detail">
                                <b>篇名：</b>《基于卷积神经网络的变电站监控图像识别方法研究》<br><b>作者：</b>赵继生<br><b>分类号：</b>TP391.41<br><b>学科专业：</b>信息与通信工程<br><b>授予学位：</b>硕士<br><b>导师姓名：</b>余萍<br><b>学位授予单位：</b>华北电力大学<br><b>学位年度：</b>2016<br><b>关键词：</b>变电站 监控视频 图像识别 卷积神经网络
                            </div>
                        </div>
                    </div>
                </div>

            </div>

        </div>
            
                <!--语句修改建议-->
        <span id="modify_suggest"></span>
        <div id="advice">
            <div class="g-line-row"></div>
            <div class="paper-txt P30 PB0">
                <div class="paper-section">
                    <p class="g-font-s16 font-bold g-font-color green MB10">该句修改建议（轻度相似，请酌情修改）</p>
                    <span class="g-font-color green"><span class="g-font-color red">常用</span>的非线性<span class="g-font-color red">激活</span><span class="g-font-color red">函数</span>有Sigmoid、ReLU等，原始的GCN采用ReLU作为<span class="g-font-color red">激活</span><span class="g-font-color red">函数</span>。</span>
                    <div class="MT10">
                        <p class="font-bold">同义词：</p>
                        <ul class="local-source-detail">
                            <li class="g-font-color green"><span class="g-font-color red">函数：</span>因变量</li>
                        </ul>

                    </div>
                </div>
            </div>
        </div>
            </div>
    <div class="back-to-top text-center">
        <a href="#gototop" class="font-bold g-font-color green">回到顶部</a>
    </div>
    <div class="paper-footer">
        <p>检测报告由<a href="http://www.paperpass.com/" target="_black">PaperPass</a>文献相似度检测系统生成</p>
        <p>Copyright © 2007-2020 PaperPass</p>
    </div>
</div>
</body>
<script type="text/javascript" src="../js/jquery.min.js"></script>
<script type="text/javascript" src="../js/Lib.js"></script>
<script type="text/javascript">
    (function(System,$){
        var tab = System.Paper.tab();
        var $advice = null;
        function run(){
            var i = 0;
            $advice.show();
            tab.call(this,{callback:function(){
                var $num = $(this).find('.chapter-num');
                if($num.length>0){
                    i++;
                    $num.text(i);
                }
            }});
            //没有内容时同时不显示该句修改建议
            if(0 === i){$advice.hide();}
        }
        $(function(){
            $advice = $('#advice');
            $(document).on('click','[tab-a="ul"] li',function(){
                run.call(this);
            });
            $(document).on('click','[tab-b="ul"] li',function(){
                run.call(this);
            });
        });
    })(Report,jQuery);


</script>
</html>
